[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiments",
    "section": "",
    "text": "Preface\nThis book contains all experiment protocols for my research on appointment scheduling.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html",
    "href": "xgboost-pairwise-ranking.html",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "",
    "text": "2.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#background",
    "href": "xgboost-pairwise-ranking.html#background",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.2 Background",
    "text": "2.2 Background\nBackground Information: To find optimal solutions for appointment scheduling problems one approach is to create local search neighborhoods and evaluate each schedule. A better search method either (1) - creates smaller search neighborhoods or (2) - evaluates faster. In this experiment we develop an Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#hypothesis",
    "href": "xgboost-pairwise-ranking.html#hypothesis",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nHypothesis: An XGBoost ranking model outperforms simple enumeration of each element of the pair.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#methodology",
    "href": "xgboost-pairwise-ranking.html#methodology",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nTools and Materials: List all tools, software, and materials needed for the experiment.\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\n\n\n\n2.4.2 Experimental Design\nDesign: We will create a random set of pairs of neighboring schedules with \\(N = 12\\) patients and \\(\\ T = 18\\) intervals of length \\(d = 5\\).\nA neighborhood consists of all schedules that differ by one patient only. Eg: ([2,1,1], [1,1,2]) are neighbors and ([2,1,1], [1,0,3]) are not.\n*Service times will have a discrete. The probability of a scheduled patient not showing up will be \\(q = 0.20\\).\n\nN = 12\nT = 18\nd = 5\ns = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1]\nq = 0.20\n\n\n\n2.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) is 1 this means that the schedule with index 1 ([1,1,2]) has the lowest objective value.\n\n\n\n2.4.4 Data Collection\nData Collection Method: The data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n2.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx 52\\ mln\\). The size of the training and test sets will be varied to explore the effect on model performance.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n2.4.6 Experimental Procedure\n\n\n\n\n\ngraph TD\n    A[\"Create features\"]:::path --&gt;|\"option 1\"| B[\"from population\"]\n    A --&gt;|\"option 2\"| C[\"random subset\"]:::path\n    B --&gt; D[\"Create pairs\"]:::path\n    C --&gt; D\n    D --&gt;|\"option 1\"| E[\"random\"]\n    D --&gt;|\"option 2\"| F[\"neighbors\"]:::path\n    E --&gt; G[\"Create labels\"]:::path\n    F --&gt; G\n    G --&gt;|\"option 1\"| H[\"objective\"]\n    G --&gt;|\"option 2\"| I[\"ranking\"]:::path\n    H --&gt; J[\"Split dataset\"]:::path\n    I --&gt; J\n    J --&gt; K[\"Train XGBoost\"]:::path\n    K --&gt; L[\"Evaluate model\"]:::path\n    \n    classDef path stroke:#f00\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import random_combination_with_replacement\n\nnum_schedules = 20000\n\nschedules = random_combination_with_replacement(T, N, num_schedules)\nfor schedule in schedules[:5]:\n    print(f\"Schedule: {schedule}\")\n\nTotal number of combinations: 51895935\nSchedule: [6, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\nSchedule: [5, 2, 3, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [5, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [7, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [5, 2, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import create_neighbors_list\n\nneighbors = create_neighbors_list(schedules)\nfor neighbor in neighbors[:5]:\n    print(f\"Neighbor: {neighbor[1]}\")\n\nNeighbor: [6, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\nNeighbor: [5, 2, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\nNeighbor: [5, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [7, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [5, 2, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective\n\nobjectives = [[calculate_objective(neighbor[0], s, d, q), calculate_objective(neighbor[1], s, d, q)] for neighbor in neighbors]\nrankings = [0 if obj[0] &lt; obj[1] else 1 for obj in objectives]\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nObjectives: [46.770431244218386, 53.202917435127475], Ranking: 0\nObjectives: [57.869940438864106, 52.16874250971341], Ranking: 1\nObjectives: [81.93609100388147, 89.24223032411952], Ranking: 0\nObjectives: [81.45210346194925, 93.75595263062714], Ranking: 0\nObjectives: [42.75077195837623, 39.65741365840285], Ranking: 1\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbor in neighbors:\n    X.append(neighbor[0] + neighbor[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=6,\n    min_child_weight=1,\n    gamma=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    learning_rate=0.1,\n    n_estimators=100,\n    early_stopping_rounds=10\n)\n\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score: {train_score:.4f}, Test Score: {test_score:.4f}\")\n\n[0] validation_0-logloss:0.52667\n[1] validation_0-logloss:0.51462\n[2] validation_0-logloss:0.50505\n[3] validation_0-logloss:0.49403\n[4] validation_0-logloss:0.47719\n[5] validation_0-logloss:0.46819\n[6] validation_0-logloss:0.45934\n[7] validation_0-logloss:0.45169\n[8] validation_0-logloss:0.44502\n[9] validation_0-logloss:0.43790\n[10]    validation_0-logloss:0.43233\n[11]    validation_0-logloss:0.42658\n[12]    validation_0-logloss:0.41747\n[13]    validation_0-logloss:0.41269\n[14]    validation_0-logloss:0.40777\n[15]    validation_0-logloss:0.40295\n[16]    validation_0-logloss:0.39807\n[17]    validation_0-logloss:0.39299\n[18]    validation_0-logloss:0.38887\n[19]    validation_0-logloss:0.38154\n[20]    validation_0-logloss:0.37742\n[21]    validation_0-logloss:0.37333\n[22]    validation_0-logloss:0.36980\n[23]    validation_0-logloss:0.36569\n[24]    validation_0-logloss:0.36179\n[25]    validation_0-logloss:0.35918\n[26]    validation_0-logloss:0.35602\n[27]    validation_0-logloss:0.35187\n[28]    validation_0-logloss:0.34834\n[29]    validation_0-logloss:0.34460\n[30]    validation_0-logloss:0.34124\n[31]    validation_0-logloss:0.33846\n[32]    validation_0-logloss:0.33569\n[33]    validation_0-logloss:0.33143\n[34]    validation_0-logloss:0.32858\n[35]    validation_0-logloss:0.32635\n[36]    validation_0-logloss:0.32196\n[37]    validation_0-logloss:0.31926\n[38]    validation_0-logloss:0.31704\n[39]    validation_0-logloss:0.31335\n[40]    validation_0-logloss:0.31197\n[41]    validation_0-logloss:0.30940\n[42]    validation_0-logloss:0.30675\n[43]    validation_0-logloss:0.30467\n[44]    validation_0-logloss:0.29993\n[45]    validation_0-logloss:0.29576\n[46]    validation_0-logloss:0.29352\n[47]    validation_0-logloss:0.29044\n[48]    validation_0-logloss:0.28854\n[49]    validation_0-logloss:0.28440\n[50]    validation_0-logloss:0.28129\n[51]    validation_0-logloss:0.27803\n[52]    validation_0-logloss:0.27645\n[53]    validation_0-logloss:0.27438\n[54]    validation_0-logloss:0.27142\n[55]    validation_0-logloss:0.26970\n[56]    validation_0-logloss:0.26762\n[57]    validation_0-logloss:0.26627\n[58]    validation_0-logloss:0.26509\n[59]    validation_0-logloss:0.26308\n[60]    validation_0-logloss:0.25970\n[61]    validation_0-logloss:0.25787\n[62]    validation_0-logloss:0.25591\n[63]    validation_0-logloss:0.25489\n[64]    validation_0-logloss:0.25222\n[65]    validation_0-logloss:0.25028\n[66]    validation_0-logloss:0.24868\n[67]    validation_0-logloss:0.24757\n[68]    validation_0-logloss:0.24448\n[69]    validation_0-logloss:0.24271\n[70]    validation_0-logloss:0.24117\n[71]    validation_0-logloss:0.24011\n[72]    validation_0-logloss:0.23773\n[73]    validation_0-logloss:0.23515\n[74]    validation_0-logloss:0.23367\n[75]    validation_0-logloss:0.23240\n[76]    validation_0-logloss:0.23169\n[77]    validation_0-logloss:0.22868\n[78]    validation_0-logloss:0.22791\n[79]    validation_0-logloss:0.22660\n[80]    validation_0-logloss:0.22425\n[81]    validation_0-logloss:0.22333\n[82]    validation_0-logloss:0.22219\n[83]    validation_0-logloss:0.22088\n[84]    validation_0-logloss:0.21910\n[85]    validation_0-logloss:0.21769\n[86]    validation_0-logloss:0.21624\n[87]    validation_0-logloss:0.21516\n[88]    validation_0-logloss:0.21412\n[89]    validation_0-logloss:0.21270\n[90]    validation_0-logloss:0.21164\n[91]    validation_0-logloss:0.21081\n[92]    validation_0-logloss:0.20994\n[93]    validation_0-logloss:0.20707\n[94]    validation_0-logloss:0.20556\n[95]    validation_0-logloss:0.20440\n[96]    validation_0-logloss:0.20197\n[97]    validation_0-logloss:0.20109\n[98]    validation_0-logloss:0.20029\n[99]    validation_0-logloss:0.19910\n[0] validation_0-logloss:0.52647\n[1] validation_0-logloss:0.51620\n[2] validation_0-logloss:0.50538\n[3] validation_0-logloss:0.48894\n[4] validation_0-logloss:0.48029\n[5] validation_0-logloss:0.47141\n[6] validation_0-logloss:0.46397\n[7] validation_0-logloss:0.45627\n[8] validation_0-logloss:0.44858\n[9] validation_0-logloss:0.44175\n[10]    validation_0-logloss:0.43581\n[11]    validation_0-logloss:0.43018\n[12]    validation_0-logloss:0.41914\n[13]    validation_0-logloss:0.41416\n[14]    validation_0-logloss:0.40876\n[15]    validation_0-logloss:0.40372\n[16]    validation_0-logloss:0.39834\n[17]    validation_0-logloss:0.39402\n[18]    validation_0-logloss:0.38945\n[19]    validation_0-logloss:0.38563\n[20]    validation_0-logloss:0.37858\n[21]    validation_0-logloss:0.37444\n[22]    validation_0-logloss:0.36732\n[23]    validation_0-logloss:0.36258\n[24]    validation_0-logloss:0.35924\n[25]    validation_0-logloss:0.35563\n[26]    validation_0-logloss:0.35247\n[27]    validation_0-logloss:0.34841\n[28]    validation_0-logloss:0.34456\n[29]    validation_0-logloss:0.34082\n[30]    validation_0-logloss:0.33775\n[31]    validation_0-logloss:0.33484\n[32]    validation_0-logloss:0.33232\n[33]    validation_0-logloss:0.32895\n[34]    validation_0-logloss:0.32568\n[35]    validation_0-logloss:0.32322\n[36]    validation_0-logloss:0.32036\n[37]    validation_0-logloss:0.31747\n[38]    validation_0-logloss:0.31523\n[39]    validation_0-logloss:0.31219\n[40]    validation_0-logloss:0.30878\n[41]    validation_0-logloss:0.30642\n[42]    validation_0-logloss:0.30419\n[43]    validation_0-logloss:0.30191\n[44]    validation_0-logloss:0.30012\n[45]    validation_0-logloss:0.29384\n[46]    validation_0-logloss:0.29204\n[47]    validation_0-logloss:0.29005\n[48]    validation_0-logloss:0.28438\n[49]    validation_0-logloss:0.28254\n[50]    validation_0-logloss:0.28029\n[51]    validation_0-logloss:0.27828\n[52]    validation_0-logloss:0.27585\n[53]    validation_0-logloss:0.27382\n[54]    validation_0-logloss:0.27166\n[55]    validation_0-logloss:0.26952\n[56]    validation_0-logloss:0.26741\n[57]    validation_0-logloss:0.26573\n[58]    validation_0-logloss:0.26175\n[59]    validation_0-logloss:0.26020\n[60]    validation_0-logloss:0.25823\n[61]    validation_0-logloss:0.25684\n[62]    validation_0-logloss:0.25424\n[63]    validation_0-logloss:0.25264\n[64]    validation_0-logloss:0.24998\n[65]    validation_0-logloss:0.24882\n[66]    validation_0-logloss:0.24747\n[67]    validation_0-logloss:0.24514\n[68]    validation_0-logloss:0.24361\n[69]    validation_0-logloss:0.24217\n[70]    validation_0-logloss:0.24055\n[71]    validation_0-logloss:0.23928\n[72]    validation_0-logloss:0.23734\n[73]    validation_0-logloss:0.23487\n[74]    validation_0-logloss:0.23305\n[75]    validation_0-logloss:0.23091\n[76]    validation_0-logloss:0.22970\n[77]    validation_0-logloss:0.22793\n[78]    validation_0-logloss:0.22714\n[79]    validation_0-logloss:0.22578\n[80]    validation_0-logloss:0.22450\n[81]    validation_0-logloss:0.22371\n[82]    validation_0-logloss:0.22281\n[83]    validation_0-logloss:0.22140\n[84]    validation_0-logloss:0.22026\n[85]    validation_0-logloss:0.21852\n[86]    validation_0-logloss:0.21749\n[87]    validation_0-logloss:0.21581\n[88]    validation_0-logloss:0.21426\n[89]    validation_0-logloss:0.21280\n[90]    validation_0-logloss:0.21134\n[91]    validation_0-logloss:0.21034\n[92]    validation_0-logloss:0.20822\n[93]    validation_0-logloss:0.20624\n[94]    validation_0-logloss:0.20419\n[95]    validation_0-logloss:0.20337\n[96]    validation_0-logloss:0.20221\n[97]    validation_0-logloss:0.20093\n[98]    validation_0-logloss:0.19999\n[99]    validation_0-logloss:0.19902\n[0] validation_0-logloss:0.52661\n[1] validation_0-logloss:0.51614\n[2] validation_0-logloss:0.50641\n[3] validation_0-logloss:0.48666\n[4] validation_0-logloss:0.47627\n[5] validation_0-logloss:0.46799\n[6] validation_0-logloss:0.45951\n[7] validation_0-logloss:0.45150\n[8] validation_0-logloss:0.43939\n[9] validation_0-logloss:0.42978\n[10]    validation_0-logloss:0.42436\n[11]    validation_0-logloss:0.41881\n[12]    validation_0-logloss:0.40987\n[13]    validation_0-logloss:0.40460\n[14]    validation_0-logloss:0.39954\n[15]    validation_0-logloss:0.39581\n[16]    validation_0-logloss:0.38952\n[17]    validation_0-logloss:0.38467\n[18]    validation_0-logloss:0.38019\n[19]    validation_0-logloss:0.37550\n[20]    validation_0-logloss:0.37111\n[21]    validation_0-logloss:0.36790\n[22]    validation_0-logloss:0.36443\n[23]    validation_0-logloss:0.36045\n[24]    validation_0-logloss:0.35667\n[25]    validation_0-logloss:0.35243\n[26]    validation_0-logloss:0.34886\n[27]    validation_0-logloss:0.34464\n[28]    validation_0-logloss:0.34208\n[29]    validation_0-logloss:0.33904\n[30]    validation_0-logloss:0.33517\n[31]    validation_0-logloss:0.33191\n[32]    validation_0-logloss:0.32875\n[33]    validation_0-logloss:0.32590\n[34]    validation_0-logloss:0.32329\n[35]    validation_0-logloss:0.32089\n[36]    validation_0-logloss:0.31885\n[37]    validation_0-logloss:0.31652\n[38]    validation_0-logloss:0.31352\n[39]    validation_0-logloss:0.31104\n[40]    validation_0-logloss:0.30817\n[41]    validation_0-logloss:0.30617\n[42]    validation_0-logloss:0.30216\n[43]    validation_0-logloss:0.29897\n[44]    validation_0-logloss:0.29532\n[45]    validation_0-logloss:0.29325\n[46]    validation_0-logloss:0.29055\n[47]    validation_0-logloss:0.28787\n[48]    validation_0-logloss:0.28514\n[49]    validation_0-logloss:0.28286\n[50]    validation_0-logloss:0.28128\n[51]    validation_0-logloss:0.27899\n[52]    validation_0-logloss:0.27544\n[53]    validation_0-logloss:0.27384\n[54]    validation_0-logloss:0.27274\n[55]    validation_0-logloss:0.27165\n[56]    validation_0-logloss:0.26943\n[57]    validation_0-logloss:0.26723\n[58]    validation_0-logloss:0.26532\n[59]    validation_0-logloss:0.26322\n[60]    validation_0-logloss:0.26104\n[61]    validation_0-logloss:0.25930\n[62]    validation_0-logloss:0.25738\n[63]    validation_0-logloss:0.25641\n[64]    validation_0-logloss:0.25479\n[65]    validation_0-logloss:0.25307\n[66]    validation_0-logloss:0.25181\n[67]    validation_0-logloss:0.25077\n[68]    validation_0-logloss:0.24949\n[69]    validation_0-logloss:0.24790\n[70]    validation_0-logloss:0.24637\n[71]    validation_0-logloss:0.24392\n[72]    validation_0-logloss:0.24213\n[73]    validation_0-logloss:0.24065\n[74]    validation_0-logloss:0.23964\n[75]    validation_0-logloss:0.23766\n[76]    validation_0-logloss:0.23403\n[77]    validation_0-logloss:0.23246\n[78]    validation_0-logloss:0.23038\n[79]    validation_0-logloss:0.22885\n[80]    validation_0-logloss:0.22773\n[81]    validation_0-logloss:0.22659\n[82]    validation_0-logloss:0.22508\n[83]    validation_0-logloss:0.22393\n[84]    validation_0-logloss:0.22260\n[85]    validation_0-logloss:0.22164\n[86]    validation_0-logloss:0.22043\n[87]    validation_0-logloss:0.21973\n[88]    validation_0-logloss:0.21881\n[89]    validation_0-logloss:0.21707\n[90]    validation_0-logloss:0.21537\n[91]    validation_0-logloss:0.21418\n[92]    validation_0-logloss:0.21260\n[93]    validation_0-logloss:0.21147\n[94]    validation_0-logloss:0.21050\n[95]    validation_0-logloss:0.20898\n[96]    validation_0-logloss:0.20740\n[97]    validation_0-logloss:0.20641\n[98]    validation_0-logloss:0.20468\n[99]    validation_0-logloss:0.20404\n[0] validation_0-logloss:0.52644\n[1] validation_0-logloss:0.51427\n[2] validation_0-logloss:0.50417\n[3] validation_0-logloss:0.48675\n[4] validation_0-logloss:0.47862\n[5] validation_0-logloss:0.47014\n[6] validation_0-logloss:0.45705\n[7] validation_0-logloss:0.45073\n[8] validation_0-logloss:0.44445\n[9] validation_0-logloss:0.43322\n[10]    validation_0-logloss:0.42622\n[11]    validation_0-logloss:0.42041\n[12]    validation_0-logloss:0.41393\n[13]    validation_0-logloss:0.40885\n[14]    validation_0-logloss:0.40369\n[15]    validation_0-logloss:0.39684\n[16]    validation_0-logloss:0.39227\n[17]    validation_0-logloss:0.38842\n[18]    validation_0-logloss:0.38250\n[19]    validation_0-logloss:0.37723\n[20]    validation_0-logloss:0.37115\n[21]    validation_0-logloss:0.36631\n[22]    validation_0-logloss:0.36270\n[23]    validation_0-logloss:0.35895\n[24]    validation_0-logloss:0.35522\n[25]    validation_0-logloss:0.35182\n[26]    validation_0-logloss:0.34879\n[27]    validation_0-logloss:0.34460\n[28]    validation_0-logloss:0.34081\n[29]    validation_0-logloss:0.33681\n[30]    validation_0-logloss:0.33415\n[31]    validation_0-logloss:0.33062\n[32]    validation_0-logloss:0.32617\n[33]    validation_0-logloss:0.32350\n[34]    validation_0-logloss:0.32081\n[35]    validation_0-logloss:0.31722\n[36]    validation_0-logloss:0.31504\n[37]    validation_0-logloss:0.31026\n[38]    validation_0-logloss:0.30834\n[39]    validation_0-logloss:0.30525\n[40]    validation_0-logloss:0.30298\n[41]    validation_0-logloss:0.30146\n[42]    validation_0-logloss:0.29949\n[43]    validation_0-logloss:0.29668\n[44]    validation_0-logloss:0.29437\n[45]    validation_0-logloss:0.28950\n[46]    validation_0-logloss:0.28678\n[47]    validation_0-logloss:0.28469\n[48]    validation_0-logloss:0.28259\n[49]    validation_0-logloss:0.28126\n[50]    validation_0-logloss:0.27915\n[51]    validation_0-logloss:0.27761\n[52]    validation_0-logloss:0.27559\n[53]    validation_0-logloss:0.27301\n[54]    validation_0-logloss:0.27111\n[55]    validation_0-logloss:0.26906\n[56]    validation_0-logloss:0.26701\n[57]    validation_0-logloss:0.26519\n[58]    validation_0-logloss:0.26203\n[59]    validation_0-logloss:0.26003\n[60]    validation_0-logloss:0.25602\n[61]    validation_0-logloss:0.25475\n[62]    validation_0-logloss:0.25193\n[63]    validation_0-logloss:0.25071\n[64]    validation_0-logloss:0.24891\n[65]    validation_0-logloss:0.24781\n[66]    validation_0-logloss:0.24682\n[67]    validation_0-logloss:0.24529\n[68]    validation_0-logloss:0.24335\n[69]    validation_0-logloss:0.24104\n[70]    validation_0-logloss:0.23938\n[71]    validation_0-logloss:0.23817\n[72]    validation_0-logloss:0.23673\n[73]    validation_0-logloss:0.23467\n[74]    validation_0-logloss:0.23117\n[75]    validation_0-logloss:0.22980\n[76]    validation_0-logloss:0.22815\n[77]    validation_0-logloss:0.22676\n[78]    validation_0-logloss:0.22522\n[79]    validation_0-logloss:0.22398\n[80]    validation_0-logloss:0.22246\n[81]    validation_0-logloss:0.22122\n[82]    validation_0-logloss:0.22026\n[83]    validation_0-logloss:0.21934\n[84]    validation_0-logloss:0.21790\n[85]    validation_0-logloss:0.21653\n[86]    validation_0-logloss:0.21514\n[87]    validation_0-logloss:0.21410\n[88]    validation_0-logloss:0.21231\n[89]    validation_0-logloss:0.21114\n[90]    validation_0-logloss:0.21029\n[91]    validation_0-logloss:0.20920\n[92]    validation_0-logloss:0.20816\n[93]    validation_0-logloss:0.20726\n[94]    validation_0-logloss:0.20642\n[95]    validation_0-logloss:0.20543\n[96]    validation_0-logloss:0.20474\n[97]    validation_0-logloss:0.20350\n[98]    validation_0-logloss:0.20234\n[99]    validation_0-logloss:0.20142\n[0] validation_0-logloss:0.52601\n[1] validation_0-logloss:0.51538\n[2] validation_0-logloss:0.50440\n[3] validation_0-logloss:0.49356\n[4] validation_0-logloss:0.48461\n[5] validation_0-logloss:0.47336\n[6] validation_0-logloss:0.46526\n[7] validation_0-logloss:0.45179\n[8] validation_0-logloss:0.44113\n[9] validation_0-logloss:0.43381\n[10]    validation_0-logloss:0.42778\n[11]    validation_0-logloss:0.42163\n[12]    validation_0-logloss:0.41066\n[13]    validation_0-logloss:0.40575\n[14]    validation_0-logloss:0.39971\n[15]    validation_0-logloss:0.39115\n[16]    validation_0-logloss:0.38615\n[17]    validation_0-logloss:0.38129\n[18]    validation_0-logloss:0.37686\n[19]    validation_0-logloss:0.37271\n[20]    validation_0-logloss:0.36870\n[21]    validation_0-logloss:0.36354\n[22]    validation_0-logloss:0.35989\n[23]    validation_0-logloss:0.35543\n[24]    validation_0-logloss:0.35138\n[25]    validation_0-logloss:0.34777\n[26]    validation_0-logloss:0.34400\n[27]    validation_0-logloss:0.34057\n[28]    validation_0-logloss:0.33689\n[29]    validation_0-logloss:0.33363\n[30]    validation_0-logloss:0.32941\n[31]    validation_0-logloss:0.32679\n[32]    validation_0-logloss:0.32423\n[33]    validation_0-logloss:0.32174\n[34]    validation_0-logloss:0.31815\n[35]    validation_0-logloss:0.31550\n[36]    validation_0-logloss:0.31299\n[37]    validation_0-logloss:0.31033\n[38]    validation_0-logloss:0.30729\n[39]    validation_0-logloss:0.30214\n[40]    validation_0-logloss:0.29976\n[41]    validation_0-logloss:0.29689\n[42]    validation_0-logloss:0.29445\n[43]    validation_0-logloss:0.29198\n[44]    validation_0-logloss:0.28688\n[45]    validation_0-logloss:0.28447\n[46]    validation_0-logloss:0.28254\n[47]    validation_0-logloss:0.27933\n[48]    validation_0-logloss:0.27734\n[49]    validation_0-logloss:0.27512\n[50]    validation_0-logloss:0.27280\n[51]    validation_0-logloss:0.27130\n[52]    validation_0-logloss:0.26924\n[53]    validation_0-logloss:0.26517\n[54]    validation_0-logloss:0.26364\n[55]    validation_0-logloss:0.26190\n[56]    validation_0-logloss:0.26016\n[57]    validation_0-logloss:0.25785\n[58]    validation_0-logloss:0.25638\n[59]    validation_0-logloss:0.25408\n[60]    validation_0-logloss:0.25132\n[61]    validation_0-logloss:0.24955\n[62]    validation_0-logloss:0.24825\n[63]    validation_0-logloss:0.24681\n[64]    validation_0-logloss:0.24497\n[65]    validation_0-logloss:0.24383\n[66]    validation_0-logloss:0.24177\n[67]    validation_0-logloss:0.24088\n[68]    validation_0-logloss:0.23878\n[69]    validation_0-logloss:0.23680\n[70]    validation_0-logloss:0.23584\n[71]    validation_0-logloss:0.23389\n[72]    validation_0-logloss:0.23307\n[73]    validation_0-logloss:0.23113\n[74]    validation_0-logloss:0.23004\n[75]    validation_0-logloss:0.22879\n[76]    validation_0-logloss:0.22774\n[77]    validation_0-logloss:0.22632\n[78]    validation_0-logloss:0.22489\n[79]    validation_0-logloss:0.22360\n[80]    validation_0-logloss:0.22250\n[81]    validation_0-logloss:0.22155\n[82]    validation_0-logloss:0.21995\n[83]    validation_0-logloss:0.21815\n[84]    validation_0-logloss:0.21686\n[85]    validation_0-logloss:0.21535\n[86]    validation_0-logloss:0.21351\n[87]    validation_0-logloss:0.21080\n[88]    validation_0-logloss:0.20959\n[89]    validation_0-logloss:0.20751\n[90]    validation_0-logloss:0.20623\n[91]    validation_0-logloss:0.20498\n[92]    validation_0-logloss:0.20390\n[93]    validation_0-logloss:0.20230\n[94]    validation_0-logloss:0.20146\n[95]    validation_0-logloss:0.20062\n[96]    validation_0-logloss:0.19968\n[97]    validation_0-logloss:0.19882\n[98]    validation_0-logloss:0.19817\n[99]    validation_0-logloss:0.19596\nFold 1 - Train Score: 0.9598, Test Score: 0.9407\nFold 2 - Train Score: 0.9567, Test Score: 0.9443\nFold 3 - Train Score: 0.9565, Test Score: 0.9470\nFold 4 - Train Score: 0.9559, Test Score: 0.9455\nFold 5 - Train Score: 0.9579, Test Score: 0.9433",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#analysis",
    "href": "xgboost-pairwise-ranking.html#analysis",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.5 Analysis",
    "text": "2.5 Analysis\nData Analysis Method: Describe the statistical or analytical methods that will be used to analyze the data.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=6,\n    min_child_weight=1,\n    gamma=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    learning_rate=0.1,\n    n_estimators=100\n)\n\nclf.fit(X, y)\n\nnum_schedules = 10\n\ntest_schedules = random_combination_with_replacement(T, N, num_schedules)\ntest_neighbors = create_neighbors_list(test_schedules)\ntest_objectives = [[calculate_objective(test_neighbor[0], s, d, q), calculate_objective(test_neighbor[1], s, d, q)] for test_neighbor in test_neighbors]\ntest_rankings = [0 if test_obj[0] &lt; test_obj[1] else 1 for test_obj in test_objectives]\n\nfor i in range(num_schedules):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {objectives[i]}, Ranking: {rankings[i]}\\n\")\n\ninput_X = test_neighbors\nX_new = []\nfor neighbor in input_X:\n    X_new.append(neighbor[0] + neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# If you want to get the probability estimates\ny_pred_proba = clf.predict_proba(X_new)\n\nprint(f\"rankings = {np.array(test_rankings)}, \\ny_pred = {y_pred}, \\ny_pred_proba = \\n{y_pred_proba}\")\n\nTotal number of combinations: 51895935\nNeighbors: ([4, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 3, 1, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [46.770431244218386, 53.202917435127475], Ranking: 0\n\nNeighbors: ([5, 4, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [6, 3, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [57.869940438864106, 52.16874250971341], Ranking: 1\n\nNeighbors: ([4, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [4, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [81.93609100388147, 89.24223032411952], Ranking: 0\n\nNeighbors: ([9, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\nObjectives: [81.45210346194925, 93.75595263062714], Ranking: 0\n\nNeighbors: ([4, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [4, 2, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\nObjectives: [42.75077195837623, 39.65741365840285], Ranking: 1\n\nNeighbors: ([8, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [8, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\nObjectives: [61.49452639027216, 67.08575383440407], Ranking: 0\n\nNeighbors: ([6, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [6, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\nObjectives: [41.4098501236233, 29.891252579712937], Ranking: 1\n\nNeighbors: ([3, 6, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 6, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\nObjectives: [57.3474750282602, 49.2705642604933], Ranking: 1\n\nNeighbors: ([4, 3, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [4, 3, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [40.49523888440565, 40.4948314280812], Ranking: 1\n\nNeighbors: ([4, 4, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 4, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [48.12597534145, 35.343204303978005], Ranking: 1\n\nrankings = [1 0 0 0 0 1 1 1 1 0], \ny_pred = [1 0 0 0 1 1 1 1 1 0], \ny_pred_proba = \n[[0.23344398 0.766556  ]\n [0.7096498  0.29035017]\n [0.6868384  0.3131616 ]\n [0.77367604 0.22632399]\n [0.20387214 0.79612786]\n [0.19631404 0.80368596]\n [0.11089581 0.8891042 ]\n [0.034863   0.965137  ]\n [0.11826545 0.88173455]\n [0.93600726 0.06399276]]\n\n\n\n2.5.1 Data Processing\nData Processing: Describe how the raw data will be processed and prepared for analysis.\n\n\n2.5.2 Statistical Tests\nStatistical Tests: List the statistical tests that will be used to test the hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#results",
    "href": "xgboost-pairwise-ranking.html#results",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.6 Results",
    "text": "2.6 Results\nExpected Results: Describe the expected outcomes or results of the experiment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#discussion",
    "href": "xgboost-pairwise-ranking.html#discussion",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.7 Discussion",
    "text": "2.7 Discussion\nDiscussion Points: Outline key points for discussion based on possible results. This may include interpretation of results, implications for theory and practice, limitations, and suggestions for future research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#timeline",
    "href": "xgboost-pairwise-ranking.html#timeline",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.8 Timeline",
    "text": "2.8 Timeline\nTimeline: This experiment was started on 25-07-2024",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#references",
    "href": "xgboost-pairwise-ranking.html#references",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.9 References",
    "text": "2.9 References\nReferences: List any references or sources cited in the protocol.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  }
]