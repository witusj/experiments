[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiments",
    "section": "",
    "text": "Preface\nThis book contains all experiment protocols for my research on appointment scheduling.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html",
    "href": "xgboost-pairwise-ranking.html",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "",
    "text": "2.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#background",
    "href": "xgboost-pairwise-ranking.html#background",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.2 Background",
    "text": "2.2 Background\nBackground Information: To find optimal solutions for appointment scheduling problems one approach is to create local search neighborhoods and evaluate each schedule. A better search method either (1) - creates smaller search neighborhoods or (2) - evaluates faster. In this experiment we develop an Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#hypothesis",
    "href": "xgboost-pairwise-ranking.html#hypothesis",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nHypothesis: An XGBoost ranking model outperforms simple evaluation of each individual element of the pair.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#methodology",
    "href": "xgboost-pairwise-ranking.html#methodology",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nTools and Materials: List all tools, software, and materials needed for the experiment.\n\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\n\n\n\n2.4.2 Experimental Design\nDesign: We will create a random set of pairs of neighboring schedules with \\(N = 12\\) patients and \\(\\ T = 18\\) intervals of length \\(d = 5\\).\nA neighborhood consists of all schedules that differ by one patient only. Eg: ([2,1,1], [1,1,2]) are neighbors and ([2,1,1], [1,0,3]) are not.\n*Service times will have a discrete. The probability of a scheduled patient not showing up will be \\(q = 0.20\\).\n\nN = 12\nT = 18\nd = 5\ns = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1]\nq = 0.20\n\n\n\n2.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) is 1 this means that the schedule with index 1 ([1,1,2]) has the lowest objective value.\n\n\n\n2.4.4 Data Collection\nData Collection Method: The data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n2.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx 52\\ mln\\). The size of the training and test sets will be varied to explore the effect on model performance.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n2.4.6 Experimental Procedure\n\n\n\n\n\ngraph TD\n    A[\"Create features\"]:::path --&gt;|\"option 1\"| B[\"from population\"]\n    A --&gt;|\"option 2\"| C[\"random subset\"]:::path\n    B --&gt; D[\"Create pairs\"]:::path\n    C --&gt; D\n    D --&gt;|\"option 1\"| E[\"random\"]\n    D --&gt;|\"option 2\"| F[\"neighbors\"]:::path\n    E --&gt; G[\"Create labels\"]:::path\n    F --&gt; G\n    G --&gt;|\"option 1\"| H[\"objective\"]\n    G --&gt;|\"option 2\"| I[\"ranking\"]:::path\n    H --&gt; J[\"Split dataset\"]:::path\n    I --&gt; J\n    J --&gt; K[\"Train XGBoost\"]:::path\n    K --&gt; L[\"Evaluate model\"]:::path\n    \n    classDef path stroke:#f00\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import random_combination_with_replacement\n\nnum_schedules = 20000\n\nstart = time.time()\nschedules = random_combination_with_replacement(T, N, num_schedules)\nfor schedule in schedules[:5]:\n    print(f\"Schedule: {schedule}\")\nend = time.time()\nprint(f\"\\nProcessing time: {end - start} seconds\\n\")\n\nTotal number of combinations: 51895935\nSchedule: [5, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\nSchedule: [6, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [6, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\nSchedule: [6, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [6, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nProcessing time: 0.22469878196716309 seconds\n\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import create_neighbors_list\n\nstart = time.time()\nneighbors = create_neighbors_list(schedules)\nfor neighbor in neighbors[:5]:\n    print(f\"Neighbor: {neighbor[1]}\")\nend = time.time()  \nprint(f\"\\nProcessing time: {end - start} seconds\\n\")\n\nNeighbor: [4, 3, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\nNeighbor: [6, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [5, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\nNeighbor: [6, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [6, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nProcessing time: 0.11014914512634277 seconds\n\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective\n\nstart = time.time()\nobjectives = [[calculate_objective(neighbor[0], s, d, q), calculate_objective(neighbor[1], s, d, q)] for neighbor in neighbors]\nrankings = [0 if obj[0] &lt; obj[1] else 1 for obj in objectives]\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\nend = time.time()\nprint(f\"\\nProcessing time: {end - start} seconds\\n\")\n\nObjectives: [54.20009059238473, 36.33404899321338], Ranking: 1\nObjectives: [71.12051634606571, 59.97814018341868], Ranking: 1\nObjectives: [49.700594275066905, 44.88077348039171], Ranking: 1\nObjectives: [56.906421689279014, 46.40882526815508], Ranking: 1\nObjectives: [67.95237356138443, 52.83133261106648], Ranking: 1\n\nProcessing time: 11.25817584991455 seconds\n\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbor in neighbors:\n    X.append(neighbor[0] + neighbor[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=6,\n    min_child_weight=1,\n    gamma=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    learning_rate=0.1,\n    n_estimators=100,\n    early_stopping_rounds=9,\n    callbacks=[CustomCallback(period=10)]\n)\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\nprint(f\"\\nProcessing time: {end - start} seconds\\n\")\n\nEpoch 9, Evaluation log: 0.43101458792760966\nEpoch 19, Evaluation log: 0.37782074872590604\nEpoch 29, Evaluation log: 0.3397793843708932\nEpoch 39, Evaluation log: 0.3079058289080858\nEpoch 49, Evaluation log: 0.2790550587377511\nEpoch 59, Evaluation log: 0.2578758574305102\nEpoch 69, Evaluation log: 0.23943076892173848\nEpoch 79, Evaluation log: 0.22511315854464192\nEpoch 89, Evaluation log: 0.20880979655025295\nEpoch 99, Evaluation log: 0.19849213470463292\nEpoch 9, Evaluation log: 0.43897900185547767\nEpoch 19, Evaluation log: 0.3794886547634378\nEpoch 29, Evaluation log: 0.33603935441747307\nEpoch 39, Evaluation log: 0.3039438911844045\nEpoch 49, Evaluation log: 0.2817735581644811\nEpoch 59, Evaluation log: 0.2615663638149854\nEpoch 69, Evaluation log: 0.24313823273591698\nEpoch 79, Evaluation log: 0.22788648836629\nEpoch 89, Evaluation log: 0.2157758003034396\nEpoch 99, Evaluation log: 0.20080993878934533\nEpoch 9, Evaluation log: 0.43440096989274024\nEpoch 19, Evaluation log: 0.38239851138927045\nEpoch 29, Evaluation log: 0.3458250670041889\nEpoch 39, Evaluation log: 0.31513877235818655\nEpoch 49, Evaluation log: 0.2913430390499998\nEpoch 59, Evaluation log: 0.2670860133811366\nEpoch 69, Evaluation log: 0.24783996737131384\nEpoch 79, Evaluation log: 0.23001038055855316\nEpoch 89, Evaluation log: 0.2157787215404096\nEpoch 99, Evaluation log: 0.20242769307695563\nEpoch 9, Evaluation log: 0.43135672983713447\nEpoch 19, Evaluation log: 0.3757831262759864\nEpoch 29, Evaluation log: 0.3355187220647931\nEpoch 39, Evaluation log: 0.30335667862696575\nEpoch 49, Evaluation log: 0.2805666941441596\nEpoch 59, Evaluation log: 0.2591551489210688\nEpoch 69, Evaluation log: 0.23745955595083068\nEpoch 79, Evaluation log: 0.2233837416851311\nEpoch 89, Evaluation log: 0.20623969742504414\nEpoch 99, Evaluation log: 0.19668674077876494\nEpoch 9, Evaluation log: 0.4340375561807305\nEpoch 19, Evaluation log: 0.37807525279931725\nEpoch 29, Evaluation log: 0.33778238695813345\nEpoch 39, Evaluation log: 0.307930760984309\nEpoch 49, Evaluation log: 0.28110420186491686\nEpoch 59, Evaluation log: 0.2622664319112664\nEpoch 69, Evaluation log: 0.24437025870743673\nEpoch 79, Evaluation log: 0.22770910403132438\nEpoch 89, Evaluation log: 0.21206607859907672\nEpoch 99, Evaluation log: 0.19907470214756903\n\nProcessing time: 0.9043898582458496 seconds\n\n\n\nStep 6: To evaluate the performance of the XGBoost model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs. The fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score: {train_score:.4f}, Test Score: {test_score:.4f}\")\n\nFold 1 - Train Score: 0.9585, Test Score: 0.9413\nFold 2 - Train Score: 0.9540, Test Score: 0.9495\nFold 3 - Train Score: 0.9557, Test Score: 0.9403\nFold 4 - Train Score: 0.9568, Test Score: 0.9477\nFold 5 - Train Score: 0.9564, Test Score: 0.9440\n\n\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=6,\n    min_child_weight=1,\n    gamma=0.1,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    learning_rate=0.1,\n    n_estimators=100\n)\n\nclf.fit(X, y)\n\nnum_schedules = 1000\n\ntest_schedules = random_combination_with_replacement(T, N, num_schedules)\ntest_neighbors = create_neighbors_list(test_schedules)\ntest_objectives = [[calculate_objective(test_neighbor[0], s, d, q), calculate_objective(test_neighbor[1], s, d, q)] for test_neighbor in test_neighbors]\ntest_rankings = [0 if test_obj[0] &lt; test_obj[1] else 1 for test_obj in test_objectives]\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# If you want to get the probability estimates\ny_pred_proba = clf.predict_proba(X_new)\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\nTotal number of combinations: 51895935\nNeighbors: ([6, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [6, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]),\nObjectives: [45.48637411924661, 42.47120095293022], Ranking: 1\n\nNeighbors: ([4, 3, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 2, 0, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\nObjectives: [40.06062082288927, 30.42619068755588], Ranking: 1\n\nNeighbors: ([5, 1, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 1, 1, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [34.58207391543724, 25.836801780752985], Ranking: 1\n\nNeighbors: ([6, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 1, 1, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [70.69619434518921, 55.23857486564482], Ranking: 1\n\nNeighbors: ([4, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [49.36070779784001, 37.83058708713856], Ranking: 1\n\nNeighbors: ([4, 3, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4, 3, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [40.898313515631344, 44.839749196115186], Ranking: 0\n\ntest_rankings = [1 1 1 1 1 0], \ny_pred = [1 1 1 1 1 0], \ny_pred_proba = \n[[0.02533388 0.9746661 ]\n [0.0144136  0.9855864 ]\n [0.11191523 0.88808477]\n [0.05962241 0.9403776 ]\n [0.0975275  0.9024725 ]\n [0.76713103 0.23286897]]\n\n\n\ndef calculate_entropies(y_pred_proba: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Calculate the entropy for each array of probabilities.\n\n    Parameters:\n    y_pred_proba (np.ndarray): Array of shape (n_samples, n_classes) with predicted probabilities for each class.\n\n    Returns:\n    np.ndarray: Array of entropies for each sample.\n    \"\"\"\n    # Ensure probabilities are in numpy array\n    y_pred_proba = np.array(y_pred_proba)\n    \n    # Calculate entropy for each array of probabilities\n    entropies = -np.sum(y_pred_proba * np.log2(y_pred_proba), axis=1)\n    \n    return entropies\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nentropies: np.ndarray = calculate_entropies(y_pred_proba)\ndf = pd.DataFrame({\"Entropy\": entropies, \"Error\": errors}).sort_values(by=\"Entropy\")\ndf['Cummulative error'] = df['Error'].cumsum()/errors.sum()\ndf.head()\n\n\n# fig = px.scatter(df, x=\"Entropy\", y=\"Cummulative error\", title=f\"Error vs Entropy&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_schedules}&lt;/sub&gt;\")\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Entropy\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\"))\nfig.add_trace(go.Scatter(x=df[\"Entropy\"], y=df[\"Cummulative error\"],\n                    mode=\"lines\",\n                    name=\"Cumm. error (normalized)\"))\nfig.update_layout(title=f\"Error vs Entropy&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_schedules}&lt;/sub&gt;\",\n                   xaxis_title=\"Entropy\",\n                   yaxis_title=\"Error\")\nfig.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#results",
    "href": "xgboost-pairwise-ranking.html#results",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.5 Results",
    "text": "2.5 Results\nExpected Results: Describe the expected outcomes or results of the experiment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#discussion",
    "href": "xgboost-pairwise-ranking.html#discussion",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.6 Discussion",
    "text": "2.6 Discussion\nDiscussion Points: Outline key points for discussion based on possible results. This may include interpretation of results, implications for theory and practice, limitations, and suggestions for future research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#timeline",
    "href": "xgboost-pairwise-ranking.html#timeline",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.7 Timeline",
    "text": "2.7 Timeline\nTimeline: This experiment was started on 25-07-2024",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#references",
    "href": "xgboost-pairwise-ranking.html#references",
    "title": "2  XGBoost model for pairwise ranking",
    "section": "2.8 References",
    "text": "2.8 References\nReferences: List any references or sources cited in the protocol.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost model for pairwise ranking</span>"
    ]
  }
]