[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiments",
    "section": "",
    "text": "Preface\nThis book contains all experiment protocols for my research on appointment scheduling.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html",
    "href": "xgboost-pairwise-ranking.html",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "",
    "text": "2.1 Objective\nObjective: Testing the performance of an XGBoost model trained for ranking pairwise schedules.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#background",
    "href": "xgboost-pairwise-ranking.html#background",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.2 Background",
    "text": "2.2 Background\nBackground Information: To find optimal solutions for appointment scheduling problems one approach is to create local search neighborhoods and evaluate the schedules in that set. A better search method either (1) - creates smaller search neighborhoods or (2) - evaluates faster. In this experiment we develop a Machine Learning model using XGBoost that can evaluate two neighboring schedules and rank them according to preference.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#hypothesis",
    "href": "xgboost-pairwise-ranking.html#hypothesis",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.3 Hypothesis",
    "text": "2.3 Hypothesis\nHypothesis: An XGBoost ranking model outperforms simple evaluation of each individual element of the pair.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#methodology",
    "href": "xgboost-pairwise-ranking.html#methodology",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.4 Methodology",
    "text": "2.4 Methodology\n\n2.4.1 Tools and Materials\nTools and Materials: We use packages from Scikit-learn to prepare training data and evaluate the model and the XGBClassifier interface from the XGBoost library.\n\nimport time\nimport math\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.base import clone\nimport xgboost as xgb\nfrom xgboost.callback import TrainingCallback\nimport plotly.graph_objects as go\n\n\n\n2.4.2 Experimental Design\nDesign: To compare an XGBoost Machine Learning model with a simple evaluation of each individual element of the pair, we will use a pairwise ranking approach. The objective is to rank two neighboring schedules according to preference.\nWe will create a random set of pairs of neighboring schedules with \\(N = 12\\) patients and \\(\\ T = 18\\) intervals of length \\(d = 5\\).\nA neighborhood consists of all schedules that differ by one patient only. Eg: ([2,1,1], [1,1,2]) are neighbors and ([2,1,1], [1,0,3]) are not.\nService times will have a discrete distribution. The probability of a scheduled patient not showing up will be \\(q = 0.20\\).\nThe objective function will be the total waiting time of all patients. The model will be trained to predict which of the two neighboring schedules has the lowest objective value. The prediction time will be recorded. Then the same schedules will be evaluated by computing the objective value and then ranked.\n\nN = 12\nT = 18\nd = 5\ns = [0.0, 0.27, 0.28, 0.2, 0.15, 0.1]\nq = 0.20\nnum_schedules = 20000\n\n\n\n2.4.3 Variables\n\nIndependent Variables: A list of tuples with pairs of neighboring schedules.\nDependent Variables: A list with rankings for each tuple of pairwise schedules. Eg: If the rank for ([2,1,1], [1,1,2]) is 1 this means that the schedule with index 1 ([1,1,2]) has the lowest objective value.\n\n\n\n2.4.4 Data Collection\nData Collection Method: The data set will be generated using simulation in which random samples will be drawn from the population of all possible schedules. For each sample a random neighboring schedule will be created.\n\n\n2.4.5 Sample Size and Selection\nSample Size: The total population size equals \\({{N + T -1}\\choose{N}} \\approx\\) 52.0 mln. For this experiment we will be using a relatively small sample of 20000 schedules.\nSample Selection: The samples will be drawn from a lexicographic order of possible schedules in order to accurately reflect the combinatorial nature of the problem and to ensure unbiased sampling from the entire combinatorial space.\n\n\n2.4.6 Experimental Procedure\nThe experiment involves multiple steps, beginning with data preparation and concluding with model evaluation. Each step offers several design options. The diagram below illustrates the sequence of steps and the available choices. We are adhering to the steps highlighted in red.\n\n\n\n\n\ngraph TD\n    A[\"Create features\"]:::path --&gt;|\"option 1\"| B[\"from population\"]\n    A --&gt;|\"option 2\"| C[\"random subset\"]:::path\n    B --&gt; D[\"Create pairs\"]:::path\n    C --&gt; D\n    D --&gt;|\"option 1\"| E[\"random\"]\n    D --&gt;|\"option 2\"| F[\"neighbors\"]:::path\n    E --&gt; G[\"Create labels\"]:::path\n    F --&gt; G\n    G --&gt;|\"option 1\"| H[\"objective\"]\n    G --&gt;|\"option 2\"| I[\"ranking\"]:::path\n    H --&gt; J[\"Split dataset\"]:::path\n    I --&gt; J\n    J --&gt; K[\"Train XGBoost\"]:::path\n    K --&gt; L[\"Evaluate model\"]:::path\n    \n    classDef path stroke:#f00\n\n\n\n\n\n\nStep 1: Randomly select a subset of schedules.\n\nfrom functions import random_combination_with_replacement\n\nstart = time.time()\nschedules = random_combination_with_replacement(T, N, num_schedules)\nprint(f\"Sampled: {len(schedules)} schedules\\n\")\nfor schedule in schedules[:5]:\n    print(f\"Schedule: {schedule}\")\nend = time.time()\ndata_prep_time = end - start\n\nprint(f\"\\nProcessing time: {data_prep_time} seconds\\n\")\n\nTotal number of combinations: 51895935\nSampled: 20000 schedules\n\nSchedule: [5, 4, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [3, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [7, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\nSchedule: [4, 4, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nSchedule: [4, 1, 3, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\nProcessing time: 0.196990966796875 seconds\n\n\n\nStep 2: Create pairs of neighboring schedules.\n\nfrom functions import create_neighbors_list\n\nstart = time.time()\nneighbors_list = create_neighbors_list(schedules)\nfor neighbor in neighbors_list[:5]:\n    print(f\"Neighbor: {neighbor[1]}\")\nend = time.time() \ntraining_set_feat_time = end - start\nprint(f\"\\nProcessing time: {training_set_feat_time} seconds\\n\")\n\nNeighbor: [5, 4, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [3, 5, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [7, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\nNeighbor: [4, 4, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nNeighbor: [3, 1, 3, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n\nProcessing time: 0.12521028518676758 seconds\n\n\n\nStep 3: For each schedule in each pair calculate the objective. For each pair save the index of the schedule that has the lowest objective value.\n\nfrom functions import calculate_objective\n\nobjectives_schedule_1 = [calculate_objective(neighbor[0], s, d, q)[0] for neighbor in neighbors_list]\nstart = time.time()\nobjectives_schedule_2 = [calculate_objective(neighbor[1], s, d, q)[0] for neighbor in neighbors_list]\nend = time.time()\ntraining_set_lab_time = end - start\nobjectives = [[obj, objectives_schedule_2[i]] for i, obj in enumerate(objectives_schedule_1)]\nrankings = [0 if obj[0] &lt; obj[1] else 1 for obj in objectives]\nfor i in range(5):\n    print(f\"Objectives: {objectives[i]}, Ranking: {rankings[i]}\")\n\n\nprint(f\"\\nProcessing time: {training_set_lab_time} seconds\\n\")\n\nObjectives: [58.64260721159582, 57.55604808663027], Ranking: 1\nObjectives: [46.193009010950895, 41.67159539988912], Ranking: 1\nObjectives: [50.46385196336541, 50.46385196336541], Ranking: 1\nObjectives: [58.25420772314158, 50.088120806882976], Ranking: 1\nObjectives: [33.711862703157664, 21.615575140770964], Ranking: 1\n\nProcessing time: 8.6248939037323 seconds\n\n\n\nStep 4: Create training and test sets.\n\n# Prepare the dataset\nX = []\nfor neighbors in neighbors_list:\n    X.append(neighbors[0] + neighbors[1])\n\nX = np.array(X)\ny = np.array(rankings)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nStep 5: Train the XGBoost model.\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B[Initialize StratifiedKFold]\n    B --&gt; C[Initialize XGBClassifier]\n    C --&gt; D[Set results as empty list]\n    D --&gt; E[Loop through each split of cv split]\n    E --&gt; F[Get train and test indices]\n    F --&gt; G[Split X and y into X_train, X_test, y_train, y_test]\n    G --&gt; H[Clone the classifier]\n    H --&gt; I[Call fit_and_score function]\n    I --&gt; J[Fit the estimator]\n    J --&gt; K[Score on training set]\n    J --&gt; L[Score on test set]\n    K --&gt; M[Return estimator, train_score, test_score]\n    L --&gt; M\n    M --&gt; N[Append the results]\n    N --&gt; E\n    E --&gt; O[Loop ends]\n    O --&gt; P[Print results]\n    P --&gt; Q[End]\n\n\n\n\n\n\n\nclass CustomCallback(TrainingCallback):\n    def __init__(self, period=10):\n        self.period = period\n\n    def after_iteration(self, model, epoch, evals_log):\n        if (epoch + 1) % self.period == 0:\n            print(f\"Epoch {epoch}, Evaluation log: {evals_log['validation_0']['logloss'][epoch]}\")\n        return False\n    \ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0\n    )\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\n# Initialize the XGBClassifier without early stopping here\n\"\"\"Load the best trial parameters from a JSON file.\"\"\"\nwith open(\"best_trial_params.json\", \"r\") as f:\n    best_trial_params = json.load(f)\n    \nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n    early_stopping_rounds=9,\n    callbacks=[CustomCallback(period=10)]\n)\n\n# clf = xgb.XGBClassifier(\n#     tree_method=\"hist\",\n#     max_depth=6,\n#     min_child_weight=1,\n#     gamma=0.1,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     learning_rate=0.1,\n#     n_estimators=100,\n#     early_stopping_rounds=9,\n#     callbacks=[CustomCallback(period=10)]\n# )\n\nstart = time.time()\nresults = []\n\nfor train_idx, test_idx in cv.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    \n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results.append((est, train_score, test_score))\nend = time.time()\ntraining_time = end - start\nprint(f\"\\nTraining time: {training_time} seconds\\n\")\n\nEpoch 9, Evaluation log: 0.38247163136303425\nEpoch 19, Evaluation log: 0.30883909496711565\nEpoch 29, Evaluation log: 0.25891160415322517\nEpoch 39, Evaluation log: 0.22431938641401938\nEpoch 49, Evaluation log: 0.19906939988950034\nEpoch 59, Evaluation log: 0.17557342481787783\nEpoch 69, Evaluation log: 0.15741070637000668\nEpoch 79, Evaluation log: 0.14500761325501663\nEpoch 89, Evaluation log: 0.13356623626250075\nEpoch 99, Evaluation log: 0.12425244349027707\nEpoch 109, Evaluation log: 0.11549353853215871\nEpoch 119, Evaluation log: 0.10753914446316776\nEpoch 129, Evaluation log: 0.10075163258190788\nEpoch 139, Evaluation log: 0.09508760424776119\nEpoch 149, Evaluation log: 0.08952930686247691\nEpoch 159, Evaluation log: 0.08478471879549579\nEpoch 169, Evaluation log: 0.08038901584507425\nEpoch 179, Evaluation log: 0.07650724647641505\nEpoch 189, Evaluation log: 0.0727816475748408\nEpoch 199, Evaluation log: 0.07032404982725074\nEpoch 209, Evaluation log: 0.0678427000084697\nEpoch 219, Evaluation log: 0.0648904362781841\nEpoch 229, Evaluation log: 0.06248534968921541\nEpoch 239, Evaluation log: 0.05987959807874176\nEpoch 249, Evaluation log: 0.05791661974263278\nEpoch 259, Evaluation log: 0.0556761164772064\nEpoch 269, Evaluation log: 0.05445275821168657\nEpoch 279, Evaluation log: 0.05328825594356387\nEpoch 289, Evaluation log: 0.05223045989313057\nEpoch 299, Evaluation log: 0.05058718744968543\nEpoch 309, Evaluation log: 0.0494937293648126\nEpoch 319, Evaluation log: 0.04759390164957674\nEpoch 329, Evaluation log: 0.0462312260530376\nEpoch 339, Evaluation log: 0.04548962091733191\nEpoch 349, Evaluation log: 0.0445395356087957\nEpoch 359, Evaluation log: 0.04392408714641852\nEpoch 369, Evaluation log: 0.04325394862160327\nEpoch 379, Evaluation log: 0.04280726971754601\nEpoch 389, Evaluation log: 0.04183886580127059\nEpoch 399, Evaluation log: 0.04121712694318231\nEpoch 409, Evaluation log: 0.04053403911883435\nEpoch 419, Evaluation log: 0.03995477157576405\nEpoch 429, Evaluation log: 0.03936940546342854\nEpoch 439, Evaluation log: 0.03903865462948969\nEpoch 449, Evaluation log: 0.03875146185423191\nEpoch 459, Evaluation log: 0.03825576012513987\nEpoch 469, Evaluation log: 0.03789307707447202\nEpoch 479, Evaluation log: 0.03758381873016799\nEpoch 489, Evaluation log: 0.03722002731425085\nEpoch 9, Evaluation log: 0.3839431093977764\nEpoch 19, Evaluation log: 0.316803259925684\nEpoch 29, Evaluation log: 0.2651792481521843\nEpoch 39, Evaluation log: 0.22738066849124153\nEpoch 49, Evaluation log: 0.20241713989035634\nEpoch 59, Evaluation log: 0.18113894148425608\nEpoch 69, Evaluation log: 0.16190695345837594\nEpoch 79, Evaluation log: 0.1459297418361748\nEpoch 89, Evaluation log: 0.13505328122454738\nEpoch 99, Evaluation log: 0.12467433972935033\nEpoch 109, Evaluation log: 0.11476937757397718\nEpoch 119, Evaluation log: 0.10705844576631535\nEpoch 129, Evaluation log: 0.1007427368335866\nEpoch 139, Evaluation log: 0.09511455561227285\nEpoch 149, Evaluation log: 0.08944775088880305\nEpoch 159, Evaluation log: 0.0852270272843557\nEpoch 169, Evaluation log: 0.08108753177426206\nEpoch 179, Evaluation log: 0.07717907239086504\nEpoch 189, Evaluation log: 0.07382146497023712\nEpoch 199, Evaluation log: 0.07120286444045973\nEpoch 209, Evaluation log: 0.06836501380605649\nEpoch 219, Evaluation log: 0.06593221980235933\nEpoch 229, Evaluation log: 0.06375260038006826\nEpoch 239, Evaluation log: 0.06204788490321741\nEpoch 249, Evaluation log: 0.06031528231051592\nEpoch 259, Evaluation log: 0.05821218106828965\nEpoch 269, Evaluation log: 0.05688165112041139\nEpoch 279, Evaluation log: 0.05508955816978477\nEpoch 289, Evaluation log: 0.05352331898263426\nEpoch 299, Evaluation log: 0.05240563311037281\nEpoch 309, Evaluation log: 0.05176925949486243\nEpoch 319, Evaluation log: 0.05028374821030723\nEpoch 329, Evaluation log: 0.04933697527826456\nEpoch 339, Evaluation log: 0.0483200848169343\nEpoch 349, Evaluation log: 0.04726855097347246\nEpoch 359, Evaluation log: 0.04623301312328684\nEpoch 369, Evaluation log: 0.04561931988285597\nEpoch 379, Evaluation log: 0.04480456585380557\nEpoch 389, Evaluation log: 0.04414834777336601\nEpoch 399, Evaluation log: 0.04392664121410476\nEpoch 409, Evaluation log: 0.04324779820138284\nEpoch 419, Evaluation log: 0.0426144003916857\nEpoch 429, Evaluation log: 0.04212930859639973\nEpoch 439, Evaluation log: 0.04150663748630479\nEpoch 449, Evaluation log: 0.04112776506838019\nEpoch 459, Evaluation log: 0.0406414802535925\nEpoch 469, Evaluation log: 0.04039199295942589\nEpoch 479, Evaluation log: 0.03994675187682309\nEpoch 489, Evaluation log: 0.03962944900992377\nEpoch 9, Evaluation log: 0.37348533668275924\nEpoch 19, Evaluation log: 0.3046779967616312\nEpoch 29, Evaluation log: 0.25925737592263615\nEpoch 39, Evaluation log: 0.22410386522312184\nEpoch 49, Evaluation log: 0.2004010506790364\nEpoch 59, Evaluation log: 0.17775708592764566\nEpoch 69, Evaluation log: 0.15846302135464793\nEpoch 79, Evaluation log: 0.14294425787671208\nEpoch 89, Evaluation log: 0.13165269420624828\nEpoch 99, Evaluation log: 0.12124356629239083\nEpoch 109, Evaluation log: 0.11240943765514748\nEpoch 119, Evaluation log: 0.1050099456749449\nEpoch 129, Evaluation log: 0.09916633815011937\nEpoch 139, Evaluation log: 0.09197318228063704\nEpoch 149, Evaluation log: 0.08724289341161626\nEpoch 159, Evaluation log: 0.08181075315717908\nEpoch 169, Evaluation log: 0.07793788953881539\nEpoch 179, Evaluation log: 0.0738678066787029\nEpoch 189, Evaluation log: 0.07071311323592408\nEpoch 199, Evaluation log: 0.06785277311144795\nEpoch 209, Evaluation log: 0.06509117262769251\nEpoch 219, Evaluation log: 0.06270335820498213\nEpoch 229, Evaluation log: 0.05997682271549661\nEpoch 239, Evaluation log: 0.05780157385005697\nEpoch 249, Evaluation log: 0.05627155431241136\nEpoch 259, Evaluation log: 0.05455188952394639\nEpoch 269, Evaluation log: 0.05335250124903269\nEpoch 279, Evaluation log: 0.05177403671947192\nEpoch 289, Evaluation log: 0.0502992349827086\nEpoch 299, Evaluation log: 0.04878120162451964\nEpoch 309, Evaluation log: 0.04750042581515347\nEpoch 319, Evaluation log: 0.04638965978969709\nEpoch 329, Evaluation log: 0.0453384065455753\nEpoch 339, Evaluation log: 0.04435587372527151\nEpoch 349, Evaluation log: 0.04371002201556358\nEpoch 359, Evaluation log: 0.04267427751428022\nEpoch 369, Evaluation log: 0.04201858106802064\nEpoch 379, Evaluation log: 0.04114477507341059\nEpoch 389, Evaluation log: 0.04039302564610673\nEpoch 399, Evaluation log: 0.03973206363263898\nEpoch 409, Evaluation log: 0.03918391616855172\nEpoch 419, Evaluation log: 0.0388662124943957\nEpoch 429, Evaluation log: 0.03847300333745497\nEpoch 439, Evaluation log: 0.0380393549977481\nEpoch 449, Evaluation log: 0.03730691660773594\nEpoch 459, Evaluation log: 0.03702111881968244\nEpoch 469, Evaluation log: 0.03664693665586536\nEpoch 479, Evaluation log: 0.03614641357609592\nEpoch 489, Evaluation log: 0.0355336605892794\nEpoch 9, Evaluation log: 0.3769457864481956\nEpoch 19, Evaluation log: 0.308107710538432\nEpoch 29, Evaluation log: 0.2611862540004076\nEpoch 39, Evaluation log: 0.22194528283609544\nEpoch 49, Evaluation log: 0.19869863549270667\nEpoch 59, Evaluation log: 0.17677496181809693\nEpoch 69, Evaluation log: 0.1588335537144885\nEpoch 79, Evaluation log: 0.14423025232226064\nEpoch 89, Evaluation log: 0.13216201160489435\nEpoch 99, Evaluation log: 0.12169889870340193\nEpoch 109, Evaluation log: 0.1137757607489366\nEpoch 119, Evaluation log: 0.10604555474537573\nEpoch 129, Evaluation log: 0.0994873592349386\nEpoch 139, Evaluation log: 0.09312401082740734\nEpoch 149, Evaluation log: 0.08770651690693194\nEpoch 159, Evaluation log: 0.08347292025588718\nEpoch 169, Evaluation log: 0.07946874758100853\nEpoch 179, Evaluation log: 0.07439421177599478\nEpoch 189, Evaluation log: 0.07203049524063647\nEpoch 199, Evaluation log: 0.06918976012726683\nEpoch 209, Evaluation log: 0.06745619420214768\nEpoch 219, Evaluation log: 0.06412788211156863\nEpoch 229, Evaluation log: 0.0621349887122783\nEpoch 239, Evaluation log: 0.06049206964974817\nEpoch 249, Evaluation log: 0.05785038131221296\nEpoch 259, Evaluation log: 0.05583208470497682\nEpoch 269, Evaluation log: 0.05437672443195849\nEpoch 279, Evaluation log: 0.05296502465485851\nEpoch 289, Evaluation log: 0.05152078793833382\nEpoch 299, Evaluation log: 0.05024785394014221\nEpoch 309, Evaluation log: 0.04945231355874114\nEpoch 319, Evaluation log: 0.04804433621779339\nEpoch 329, Evaluation log: 0.04648147120935184\nEpoch 339, Evaluation log: 0.04579441847542242\nEpoch 349, Evaluation log: 0.04491691003664874\nEpoch 359, Evaluation log: 0.04373690498738476\nEpoch 369, Evaluation log: 0.04302180119660257\nEpoch 379, Evaluation log: 0.04263129232832263\nEpoch 389, Evaluation log: 0.04227804737436229\nEpoch 399, Evaluation log: 0.04162001935396698\nEpoch 409, Evaluation log: 0.04073367187263532\nEpoch 419, Evaluation log: 0.04045577921699257\nEpoch 429, Evaluation log: 0.03975970771514342\nEpoch 439, Evaluation log: 0.03960809067929816\nEpoch 449, Evaluation log: 0.03917769643831882\nEpoch 459, Evaluation log: 0.03878489302925228\nEpoch 469, Evaluation log: 0.03843714174453418\nEpoch 479, Evaluation log: 0.03813844806505952\nEpoch 489, Evaluation log: 0.03778641245441142\nEpoch 9, Evaluation log: 0.3783941830438562\nEpoch 19, Evaluation log: 0.30222505612811074\nEpoch 29, Evaluation log: 0.25763539372221567\nEpoch 39, Evaluation log: 0.22342197418748402\nEpoch 49, Evaluation log: 0.1961129531635088\nEpoch 59, Evaluation log: 0.17787778443063143\nEpoch 69, Evaluation log: 0.1588983331694617\nEpoch 79, Evaluation log: 0.1447804333538661\nEpoch 89, Evaluation log: 0.13302366530573637\nEpoch 99, Evaluation log: 0.12258731623091444\nEpoch 109, Evaluation log: 0.11318914428677818\nEpoch 119, Evaluation log: 0.10711135420747268\nEpoch 129, Evaluation log: 0.10036194257971329\nEpoch 139, Evaluation log: 0.09493543367298207\nEpoch 149, Evaluation log: 0.08982105537637744\nEpoch 159, Evaluation log: 0.085427703416452\nEpoch 169, Evaluation log: 0.08029012639213852\nEpoch 179, Evaluation log: 0.0765718674413597\nEpoch 189, Evaluation log: 0.07280729694384036\nEpoch 199, Evaluation log: 0.07017471769924055\nEpoch 209, Evaluation log: 0.06675714379087117\nEpoch 219, Evaluation log: 0.06435183930956447\nEpoch 229, Evaluation log: 0.06204230466204996\nEpoch 239, Evaluation log: 0.05983427265596717\nEpoch 249, Evaluation log: 0.05794039276605139\nEpoch 259, Evaluation log: 0.05657029113038796\nEpoch 269, Evaluation log: 0.05458827512370304\nEpoch 279, Evaluation log: 0.053130550902169\nEpoch 289, Evaluation log: 0.05193880843819029\nEpoch 299, Evaluation log: 0.05098533582342936\nEpoch 309, Evaluation log: 0.0493940681301468\nEpoch 319, Evaluation log: 0.04851033938476222\nEpoch 329, Evaluation log: 0.04743886737337446\nEpoch 339, Evaluation log: 0.04658280349197113\nEpoch 349, Evaluation log: 0.045829029025468\nEpoch 359, Evaluation log: 0.04525945140120478\nEpoch 369, Evaluation log: 0.04425408834898464\nEpoch 379, Evaluation log: 0.04360489736326378\nEpoch 389, Evaluation log: 0.04316306252079627\nEpoch 399, Evaluation log: 0.04278075932533008\nEpoch 409, Evaluation log: 0.04210342358255148\nEpoch 419, Evaluation log: 0.04158380164940008\nEpoch 429, Evaluation log: 0.04097901457186137\nEpoch 439, Evaluation log: 0.04045562337559407\nEpoch 449, Evaluation log: 0.04032690818932414\nEpoch 459, Evaluation log: 0.0399386553731102\nEpoch 469, Evaluation log: 0.03941659511990367\nEpoch 479, Evaluation log: 0.03899142205826735\nEpoch 489, Evaluation log: 0.0384332001006594\n\nTraining time: 3.1231930255889893 seconds\n\n\n\nStep 6: To evaluate the performance of the XGBoost ranking model, we will use Stratified K-Fold Cross-Validation with 5 splits, ensuring each fold maintains the same class distribution as the original dataset. Using StratifiedKFold(n_splits=5, shuffle=True, random_state=94), the dataset will be divided into five folds. In each iteration, the model will be trained on four folds and evaluated on the remaining fold. A custom callback, CustomCallback(period=10), will print the evaluation log every 10 epochs.\nThe fit_and_score function will fit the model and score it on both the training and test sets, storing the results for each fold. This provides insight into the model’s performance across different subsets of the data, helps in understanding how well the model generalizes to unseen data and identifies potential overfitting or underfitting issues. The overall processing time for the cross-validation will also be recorded.\n\n# Print results\nfor i, (est, train_score, test_score) in enumerate(results):\n    print(f\"Fold {i+1} - Train Score: {train_score:.4f}, Test Score: {test_score:.4f}\")\n\nFold 1 - Train Score: 0.9992, Test Score: 0.9920\nFold 2 - Train Score: 0.9993, Test Score: 0.9908\nFold 3 - Train Score: 0.9993, Test Score: 0.9915\nFold 4 - Train Score: 0.9995, Test Score: 0.9915\nFold 5 - Train Score: 0.9994, Test Score: 0.9912\n\n\nTraining the model on the entire dataset provides a final model that has learned from all available data. Recording the training time helps in understanding the computational efficiency and scalability of the model with the given hyperparameters.\n\n# Fit the model on the entire dataset\n# Initialize the XGBClassifier without early stopping here\n\nstart = time.time()\n\nclf = xgb.XGBClassifier(\n    tree_method=\"hist\",\n    max_depth=best_trial_params[\"max_depth\"],\n    min_child_weight=best_trial_params[\"min_child_weight\"],\n    gamma=best_trial_params[\"gamma\"],\n    subsample=best_trial_params[\"subsample\"],\n    colsample_bytree=best_trial_params[\"colsample_bytree\"],\n    learning_rate=best_trial_params[\"learning_rate\"],\n    n_estimators=best_trial_params[\"n_estimators\"],\n)\n\nclf.fit(X, y)\nend= time.time()\nmodeling_time = end - start\nprint(f\"\\nTraining time: {modeling_time} seconds\\n\")\n\n\nTraining time: 0.32645487785339355 seconds\n\n\n\nGenerating test schedules and calculating their objectives and rankings allows us to create a new dataset for evaluating the model’s performance on unseen data.\n\nnum_test_schedules = 1000\n\ntest_schedules = random_combination_with_replacement(T, N, num_test_schedules)\ntest_neighbors = create_neighbors_list(test_schedules)\n\nprint(f\"Sampled: {len(test_schedules)} schedules\\n\")\n\ntest_objectives_schedule_1 = [calculate_objective(test_neighbor[0], s, d, q)[0] for test_neighbor in test_neighbors]\n\n# Start time measeurement for the evaluation\nstart = time.time()\ntest_objectives_schedule_2 = [calculate_objective(test_neighbor[1], s, d, q)[0] for test_neighbor in test_neighbors]\ntest_rankings = [0 if test_obj &lt; test_objectives_schedule_2[i] else 1 for i, test_obj in enumerate(test_objectives_schedule_1)]\nend = time.time()\nevaluation_time = end - start\n\n# Combine the objectives for each pair for later processing\ntest_objectives = [[test_obj, test_objectives_schedule_2[i]] for i, test_obj in enumerate(test_objectives_schedule_1)]\n\nprint(f\"\\nEvaluation time: {evaluation_time} seconds\\n\")\n\nfor i in range(6):\n    print(f\"Neighbors: {test_neighbors[i]},\\nObjectives: {test_objectives[i]}, Ranking: {test_rankings[i]}\\n\")\n\nTotal number of combinations: 51895935\nSampled: 1000 schedules\n\n\nEvaluation time: 0.47791004180908203 seconds\n\nNeighbors: ([4, 3, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [3, 3, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]),\nObjectives: [48.40501938853018, 32.607099352902864], Ranking: 1\n\nNeighbors: ([5, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [5, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\nObjectives: [43.759717219472705, 36.89206532269476], Ranking: 1\n\nNeighbors: ([5, 2, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [5, 2, 2, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [43.84592883469714, 43.799233367301944], Ranking: 1\n\nNeighbors: ([5, 2, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [5, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\nObjectives: [44.74318754312179, 36.02999428630489], Ranking: 1\n\nNeighbors: ([5, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [27.492704547223514, 24.95484792883403], Ranking: 1\n\nNeighbors: ([7, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\nObjectives: [72.89801483197238, 60.414264042664364], Ranking: 1\n\n\n\nMaking predictions on new data and comparing them to the actual rankings provides an evaluation of the model’s performance in practical applications. Recording the prediction time helps in understanding the model’s efficiency during inference.\n\ninput_X = test_neighbors\nX_new = []\nfor test_neighbor in input_X:\n    X_new.append(test_neighbor[0] + test_neighbor[1])\n    \n# Predict the target for new data\ny_pred = clf.predict(X_new)\n\n# Probability estimates\nstart = time.time()\ny_pred_proba = clf.predict_proba(X_new)\nend = time.time()\nprediction_time = end - start\nprint(f\"\\nPrediction time: {prediction_time} seconds\\n\")\n\nprint(f\"test_rankings = {np.array(test_rankings)[:6]}, \\ny_pred = {y_pred[:6]}, \\ny_pred_proba = \\n{y_pred_proba[:6]}\")\n\n\nPrediction time: 0.005075931549072266 seconds\n\ntest_rankings = [1 1 1 1 1 1], \ny_pred = [1 1 1 1 1 1], \ny_pred_proba = \n[[1.1920929e-07 9.9999988e-01]\n [1.4652014e-03 9.9853480e-01]\n [5.2697659e-03 9.9473023e-01]\n [2.0772815e-03 9.9792272e-01]\n [5.2012205e-03 9.9479878e-01]\n [4.5244098e-03 9.9547559e-01]]\n\n\nCalculating the ambiguousness of the predicted probabilities helps in understanding the model’s confidence in its predictions. High ambiguousness indicates uncertain predictions, while low ambiguousness indicates confident predictions.\nCalculating cumulative error rate and cumulative accuracy helps in understanding how the model’s performance evolves over the dataset.\nVisualizing the relationship between ambiguousness and error provides insights into how uncertainty in the model’s predictions correlates with its accuracy. This can help in identifying patterns and understanding the conditions under which the model performs well or poorly.\n\nfrom functions import calculate_ambiguousness\n\nerrors = np.abs(y_pred - np.array(test_rankings))\nambiguousness: np.ndarray = calculate_ambiguousness(y_pred_proba)\ndf = pd.DataFrame({\"Ambiguousness\": ambiguousness, \"Error\": errors}).sort_values(by=\"Ambiguousness\")\ndf['Cumulative error rate'] = df['Error'].expanding().mean()\n# Calculate cumulative accuracy\ndf['Cumulative accuracy'] = 1 - df['Cumulative error rate']\ndf.head()\n\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Error\"],\n                    mode=\"markers\",\n                    name=\"Error\",\n                    marker=dict(size=9)))\nfig.add_trace(go.Scatter(x=df[\"Ambiguousness\"], y=df[\"Cumulative accuracy\"],\n                    mode=\"lines\",\n                    name=\"Cum. accuracy\",\n                    line = dict(width = 3, dash = 'dash')))\nfig.update_layout(title=f\"Error vs Ambiguousness&lt;/br&gt;&lt;/br&gt;&lt;sub&gt;n={num_test_schedules}&lt;/sub&gt;\",\n                   xaxis_title=\"Ambiguousness\",\n                   yaxis_title=\"Error / Accuracy\",\n                   hoverlabel=dict(font=dict(color='white')))\nfig.show()\n\n/Users/witoldtenhove/Documents/Projects/vu/appointment-scheduling/book/functions.py:213: RuntimeWarning:\n\ndivide by zero encountered in log2\n\n/Users/witoldtenhove/Documents/Projects/vu/appointment-scheduling/book/functions.py:213: RuntimeWarning:\n\ninvalid value encountered in multiply",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#results",
    "href": "xgboost-pairwise-ranking.html#results",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.5 Results",
    "text": "2.5 Results\nResults: We wanted to test whether an XGBoost classification model could be used to assess and rank the quality of pairs of schedules. For performance benchmarking we use the conventional calculation method using Lindley recursions.\nWe trained the XGBoost ranking model using a limited set of features (schedules) and labels (objectives). The total number of possible schedules is approximately 52.0 million. For training and validation, we used a sample of 20000 schedules. Generating the feature and label set took a total of 8.9471 seconds, with the calculation of objective values accounting for 8.6249 seconds.\nThe model demonstrates strong and consistent performance with high accuracies both for training as well as testing, good generalization and stability. Total training time for the final model was 0.3265 seconds. The evaluation of 1000 test schedules took 0.0051 seconds for the the XGBoost model and 0.4779 for the conventional method, which is an improvement of 94X.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#discussion",
    "href": "xgboost-pairwise-ranking.html#discussion",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.6 Discussion",
    "text": "2.6 Discussion\nDiscussion Points: In this experiment we used total waiting time as the objective value. In conventional appointment schedule problems the objective function also includes the physician’s idle time and overtime.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#timeline",
    "href": "xgboost-pairwise-ranking.html#timeline",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.7 Timeline",
    "text": "2.7 Timeline\nTimeline: This experiment was started on 25-07-2024",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  },
  {
    "objectID": "xgboost-pairwise-ranking.html#references",
    "href": "xgboost-pairwise-ranking.html#references",
    "title": "2  XGBoost classification model for pairwise ranking",
    "section": "2.8 References",
    "text": "2.8 References\nReferences: List any references or sources cited in this report.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>XGBoost classification model for pairwise ranking</span>"
    ]
  }
]